{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9db46f-49e7-4d7f-a825-153610594b8a",
   "metadata": {},
   "source": [
    "## ETL using Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ba23e-2281-460b-92aa-68fe430df558",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "  <li>\n",
    "    <a href=\"#Objectives\">Objectives\n",
    "    </a>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Datasets\">Datasets\n",
    "    </a>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Setup\">Setup\n",
    "    </a>\n",
    "    <ol>\n",
    "      <li>\n",
    "        <a href=\"#Installing-Required-Libraries\">Installing Required Libraries\n",
    "        </a>\n",
    "      </li>\n",
    "      <li>\n",
    "        <a href=\"#Importing-Required-Libraries\">Importing Required Libraries\n",
    "        </a>\n",
    "      </li>\n",
    "    </ol>\n",
    "  </li>\n",
    "  <li> \n",
    "    <a href=\"#Examples\">Examples\n",
    "    </a>\n",
    "    <ol>\n",
    "    <li>\n",
    "      <a href=\"#Task-1---Create-a-Dataframe-from-the-raw-data-and-write-to-CSV-file.\">Task 1 - Create a Dataframe from the raw data and write to CSV file.\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-2---Read-from-a-csv-file-and-write-to-parquet-file\">Task 2 - Read from a csv file and write to parquet file\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-3---Condense-PARQUET-to-a-single-file.\">Task 3 - Condense PARQUET to a single file.\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-4---Read-from-a-parquet-file-and-write-to-csv-file\">Task 4 - Read from a parquet file and write to csv file\n",
    "      </a>\n",
    "    </li>\n",
    "      </ol>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8524ffc0-1d5b-4cee-b2ec-8f2c03120548",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    " \n",
    " - Create a Spark Dataframe from the raw data and write to CSV file.\n",
    " - Read from a csv file and write to parquet file\n",
    " - Condense PARQUET to a single file.\n",
    " - Read from a parquet file and write to csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9790976f-6a35-4007-83bd-2394aa3a7702",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d1518-dcb3-4eba-972a-7bc7beb1041c",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed81c88-c463-466b-bc6c-66a3c4f0803f",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01) for connecting to the Spark Cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d318c-6121-4d8c-96a1-be5e3bce4cd1",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.\n",
    "\n",
    "If you wish to download this jupyter notebook and run on your local computer, follow the instructions mentioned <a href=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/labs/Connecting_to_spark_cluster_using_Skills_Network_labs.ipynb\">here.</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca314e5-0ca3-4051-870d-b1b5c93bafb4",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683484a-bc5d-4a5a-b316-3e14a20c6842",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.1.2 -q\n",
    "!pip install findspark -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda45c0-f63b-4d5b-8a8d-b4d40ada0eab",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae96efe-59b4-4518-aebc-03acc785df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221f638-e423-4f3a-bebf-97579a99d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "#Ignore any warnings by SparkSession command\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETL using Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcfc726-8b8e-4e5f-b279-5eb556bb1e78",
   "metadata": {},
   "source": [
    "## Task 1 - Create a Dataframe from the raw data and write to CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18d9bf-9356-44ba-84a1-18e01f7d96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of tuples\n",
    "#each tuple contains the student id, height and weight\n",
    "data = [(\"student1\",64,90),\n",
    "        (\"student2\",59,100),\n",
    "        (\"student3\",69,95),\n",
    "        (\"\",70,110),\n",
    "        (\"student5\",60,80),\n",
    "        (\"student3\",69,95),\n",
    "        (\"student6\",62,85),\n",
    "        (\"student7\",65,80),\n",
    "        (\"student7\",65,80)]\n",
    "\n",
    "# some rows are intentionally duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b35f73-9509-4ada-bef9-88fb0db3901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe using createDataFrame and pass the data and the column names.\n",
    "\n",
    "df = spark.createDataFrame(data, [\"student\",\"height_inches\",\"weight_pounds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a617be0-8ccc-4a4f-9eb5-893baa8c8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the data frame\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61fc1b5-6089-405e-aefc-7fcd634a822e",
   "metadata": {},
   "source": [
    "Write to csv file\n",
    "\n",
    ">**Note: In Apache Spark, when you use the write method to save a DataFrame to a CSV file, it indeed creates a directory rather than a single file. This is because Spark is designed to run in a distributed manner across multiple nodes, and it saves the output as multiple part files within a directory.The csv file is within the directory.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bddd350-9999-475a-8e3a-2e1c722fe302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").csv(\"student-hw.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc83d211-64fc-49b6-a20d-481edc7673f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you do not wish to over write use df.write.csv(\"student-hw.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b5dc8-5271-4ff8-996e-4bdbc2eafbb3",
   "metadata": {},
   "source": [
    "Verify the csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35a2a4-f081-409a-a9f5-a40363551888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student-hw.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140750c7-3eff-48eb-afb4-a83c25ef10e0",
   "metadata": {},
   "source": [
    "## Task 2 - Read from a csv file and write to parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50ece9-933c-465d-89f3-803c03d5f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student-hw.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d0ede-3194-4554-960a-9648ee53517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of rows in the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4aaab8-5626-4a1a-a822-ed1f6c1b9e5b",
   "metadata": {},
   "source": [
    "Drop Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10bb50-9497-4b0d-a22a-e5e1962ca26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512b385d-4fb6-4aac-8450-53825a00dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba7ad7-ed48-4bd4-856c-a905f115b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice that the duplicates are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1628eb-342f-441d-b4ee-052aecf149d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of rows in the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78381bfc-11e8-4166-be5a-b442c7ecbe68",
   "metadata": {},
   "source": [
    "Drop Null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669f402-983e-46f0-a105-3dc72d6524df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf63a9-e1c2-4a85-9f9f-4ce6584d0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observe the rows with null values getting dropped\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ced46-2306-4f0c-9f70-f6cd271f3502",
   "metadata": {},
   "source": [
    "Save to parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef542589-c2ac-4b56-a3e0-d4f2e6210057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the data to a Parquet file\n",
    "df.write.mode(\"overwrite\").parquet(\"student-hw.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9a989-8bda-4379-b148-aa054468e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you do not wish to overwrite use the command df.write.parquet(\"student-hw.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e41cf9-75cc-4167-8150-932fd33d8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the parquet file(s) are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1844c48-5099-4cd1-8399-ebab5b038641",
   "metadata": {},
   "outputs": [],
   "source": [
    "!!ls -l student-hw.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d704f-db7b-4c08-94c4-ce65ad4dbe45",
   "metadata": {},
   "source": [
    "Notice that there are a lot of .parquet files in the output.\n",
    "- To improve parallellism, spark stores each dataframe in multiple partitions.\n",
    "- When the data is saved as parquet file, each partition is saved as a separate file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba7853-135d-48e6-bfa2-870a9e3f8d94",
   "metadata": {},
   "source": [
    "## Task 3 - Condense PARQUET to a single file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9462ed93-f581-4521-adc0-d69997f4da15",
   "metadata": {},
   "source": [
    "Reduce the number of partitions in the dataframe to one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1372b955-cfe6-4066-8601-f8c6c5160caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a6115-ae58-42cc-80ff-c0bb30aa743c",
   "metadata": {},
   "source": [
    "Save to parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f6e00-ea6c-4edb-add1-18b62737bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the data to a Parquet file\n",
    "df.write.mode(\"overwrite\").parquet(\"student-hw-single.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4cffa-caac-46f6-b9ee-6feee9404ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you do not wish to overwrite use the command df.write.parquet(\"student-hw-single.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372309df-c571-4438-8c14-cbd0a6e91583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the parquet file(s) are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581694b-dba2-45ab-92cf-8bc7f41e5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l student-hw-single.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1442e-8722-4723-b1ef-e4313fb48cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice that there is only one .parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3bdd1-638b-48e4-8cdd-423567b7e1ef",
   "metadata": {},
   "source": [
    "## Task 4 - Read from a parquet file and write to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf63d8e6-232b-44ed-b3c5-5519ce705ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"student-hw-single.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5151b6-8927-4087-9834-e1ff89ee4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77299e50-85b6-4f6b-99fc-6b0580a158be",
   "metadata": {},
   "source": [
    "Transform the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f1b91-eeea-4e6d-88ac-d0c2f5277d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the expr function that helps in transforming the data\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88687a-0955-489e-9f72-f94fe446d9b1",
   "metadata": {},
   "source": [
    "Convert inches to centimeters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914cb68-42e7-4ca8-afaa-9b3e59ca76f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inches to centimeters\n",
    "# Multiply the column height_inches with 2.54 to get a new column height_centimeters\n",
    "df = df.withColumn(\"height_centimeters\", expr(\"height_inches * 2.54\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203adfa-5eda-4a16-aed0-c114e7c37265",
   "metadata": {},
   "source": [
    "Convert pounds to kilograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f135234-6e93-4d45-a715-9d5c7fc9fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pounds to kilograms\n",
    "# Multiply weight_pounds with 0.453592 to get a new column weight_kg\n",
    "df = df.withColumn(\"weight_kg\", expr(\"weight_pounds * 0.453592\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94f65c-f164-4b8e-8d66-94952abef6d6",
   "metadata": {},
   "source": [
    "Drop the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab2c32-cd80-4893-ac0b-857a6bfb3f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns \"height_inches\",\"weight_pounds\"\n",
    "df = df.drop(\"height_inches\",\"weight_pounds\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb2bdc-5791-463a-8ed6-b44d327e5419",
   "metadata": {},
   "source": [
    "Rename a column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c89b7-a1c7-4c0e-a056-dd3b81a9e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the lengthy column name \"height_centimeters\" to \"height_cm\"\n",
    "df = df.withColumnRenamed(\"height_centimeters\",\"height_cm\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f105bdb-7d25-457a-a85a-c63575c45822",
   "metadata": {},
   "source": [
    "Save to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5440c-107a-402f-9dcb-5425c7ce3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").csv(\"student_transformed.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723e0c0-5404-4b13-a4d5-1edc64241899",
   "metadata": {},
   "source": [
    "Verify the csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ceee81-6eba-4aa4-a123-6bc78bc9305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student_transformed.csv\", header=True, inferSchema=True)\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a31447-f41a-4824-9f9f-b132cc6de033",
   "metadata": {},
   "source": [
    "Stop Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb210429-1216-4725-993f-ac0bedf88bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1209801-3436-4192-a9c6-96539a6750a1",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb76791-ec32-4b22-8304-4b1590053449",
   "metadata": {},
   "source": [
    "<!--\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2023-05-21|0.1|Ramesh Sannareddy|Initial Version Created|\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "39731b452d2aa53db85966a6e8def5ff45a8e2114b7123f9ebeb021e9b376712"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
