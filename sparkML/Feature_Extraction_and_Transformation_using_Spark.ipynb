{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2dbb3d-ba42-4c99-813e-b35e75dff261",
   "metadata": {},
   "source": [
    "## Feature Extraction and Transformation using Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb376f27-96cf-4743-8cda-31878b745b69",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Use the feature extractor CountVectorizer\n",
    " - Use the feature extractor TF-IDF\n",
    " - Use the feature transformer Tokenizer\n",
    " - Use the feature transformer StopWordsRemover\n",
    " - Use the feature transformer StringIndexer\n",
    " - Use the feature transformer StandardScaler\n",
    "\n",
    "Feature extraction and transformation are crucial components of data preprocessing.\n",
    "Feature extraction involves selecting or deriving significant features from raw data for analysis or modeling purposes.\n",
    "\n",
    "Spark MLlib provides various feature extraction techniques such as Tokenization,TF-IDF, Word2Vec, and PCA.\n",
    "\n",
    "Feature transformation involves converting extracted features into a suitable format for subsequent analysis.\n",
    "\n",
    "Spark offers commonly used feature transformation techniques like scaling and normalization, one-hot encoding, StringIndexer, and PCA.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada69e88-4a55-4c0e-864c-8d7ceae77ea5",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "In this lab you will be using dataset(s):\n",
    "\n",
    " - Modified version of car mileage dataset. Original dataset available at https://archive.ics.uci.edu/ml/datasets/auto+mpg \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304e7c5-5547-4d4c-8b4b-eb1b3ee1846c",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11d4ab-ea08-4625-a674-22d02c69d7d0",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35598e8f-e178-4a42-b387-13a91c4bb8e4",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01) for connecting to the Spark Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb47fa-142a-4f93-af1d-2af440df6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc3d77-ae3d-4b72-9c78-84f7701b4838",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfee36-e5d1-4be7-95d1-f19f72a17f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Enable Python faulthandler for better tracebacks\n",
    "spark = SparkSession.builder.appName(\"Feature Extraction and Transformation using Spark\") \\\n",
    "    .config(\"spark.sql.execution.pyspark.udf.faulthandler.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.faulthandler.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b4b9c-074c-405e-8b94-883ff7807aab",
   "metadata": {},
   "source": [
    "## Task 1 - Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597526b-465c-4c3f-b6cd-d4c6d6470819",
   "metadata": {},
   "source": [
    "A tokenizer is used to break a sentence into words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b57ac5-8fa9-400f-a0cf-8e5af01b70b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tokenizer\n",
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8607212-8427-45e3-866c-aee7e10225d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sample dataframe\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (1, \"Spark is a distributed computing system.\"),\n",
    "    (2, \"It provides interfaces for multiple languages\"),\n",
    "    (3, \"Spark is built on top of Hadoop\")], [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3e409-b39c-4f4b-b806-279cd8a574d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the dataframe\n",
    "sentenceDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3556d0-c980-4c7a-905d-46983bb22ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokenizer instance.\n",
    "#mention the column to be tokenized as inputcol\n",
    "#mention the output column name where the tokens are to be stored.\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd8280-5ebb-4a86-ae97-e9fd8c08a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "token_df = tokenizer.transform(sentenceDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ecc22e-6860-4031-888c-b6b244c77ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the tokenized data\n",
    "token_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387cb147-cf6a-4506-a99b-3ae8e035c9bb",
   "metadata": {},
   "source": [
    "## Task 2 - CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfbd665-21d6-4759-9e3f-ca214e2b7157",
   "metadata": {},
   "source": [
    "CountVectorizer is used to convert text into numerical format. It gives the count of each word in a given document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a6840-cbdd-4ccb-8d96-d07373c06111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import CountVectorizer\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391481bc-98e7-4f02-8bf7-7c85b149e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sample dataframe and display it.\n",
    "textdata = [(1, \"I love Spark Spark provides Python API \".split()),\n",
    "            (2, \"I love Python Spark supports Python\".split()),\n",
    "            (3, \"Spark solves the big problem of big data\".split())]\n",
    "\n",
    "textdata = spark.createDataFrame(textdata, [\"id\", \"words\"])\n",
    "\n",
    "textdata.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc23e6c-a55c-449f-8f61-781c7f09df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "# mention the column to be count vectorized as inputcol\n",
    "# mention the output column name where the count vectors are to be stored.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a7ddd-b46c-4c3c-a6fb-1ff9f038b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the CountVectorizer model on the input data\n",
    "model = cv.fit(textdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89ab44-bd66-45e5-bbb7-97d9afb43185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the input data to bag-of-words vectors\n",
    "result = model.transform(textdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d8c7b-9b05-4ca6-a678-bf5d99c2a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataframe\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b60073-5af4-4e24-bb2d-f6845c9a0a5b",
   "metadata": {},
   "source": [
    "## Task 3 - TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebacae5b-d5a0-438e-a841-a6982a912999",
   "metadata": {},
   "source": [
    "Term Frequency-Inverse Document Frequency is used to quantify the importance of a word in a document. TF-IDF is computed by multiplying the number of times a word occurs in a document by the inverse document frequency of the word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcbc9a-e4bf-49b8-866a-31b23c51a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary classes for TF-IDF calculation\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9434ab-e521-4995-b5c3-0ca7ca4ff1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sample dataframe and display it.\n",
    "sentenceData = spark.createDataFrame([\n",
    "        (1, \"Spark supports python\"),\n",
    "        (2, \"Spark is fast\"),\n",
    "        (3, \"Spark is easy\")\n",
    "    ], [\"id\", \"sentence\"])\n",
    "\n",
    "sentenceData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad3d3a-d77d-4f55-9268-0d3349502dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the \"sentence\" column and store in the column \"words\"\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829421a-c1c0-4f6e-8f38-ed9e0dacac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HashingTF object\n",
    "# mention the \"words\" column as input\n",
    "# mention the \"rawFeatures\" column as output\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "featurizedData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7b477-1489-4d9b-b2ce-e37c6a3f6782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an IDF object\n",
    "# mention the \"rawFeatures\" column as input\n",
    "# mention the \"features\" column as output\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "tfidfData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e4aee-8816-403b-b582-84e1b4e4bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the tf-idf data\n",
    "tfidfData.select(\"sentence\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2699d-314e-4648-b53b-9b1623f66c27",
   "metadata": {},
   "source": [
    "## Task 4 - StopWordsRemover\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be3fd2-8ba7-44e6-9c20-2aa0c10d7640",
   "metadata": {},
   "source": [
    "StopWordsRemover is a transformer that filters out stop words like \"a\",\"an\" and \"the\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aaedb0-bdcd-40c8-9662-a90e1d96f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import StopWordsRemover\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce160df-aaf6-4040-9b0d-75ca25085fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with sample text and display it\n",
    "textData = spark.createDataFrame([\n",
    "    (1, ['Spark', 'is', 'an', 'open-source', 'distributed', 'computing', 'system']),\n",
    "    (2, ['IT', 'has', 'interfaces', 'for', 'multiple', 'languages']),\n",
    "    (3, ['It', 'has', 'a', 'wide', 'range', 'of', 'libraries', 'and', 'APIs'])\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "textData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759ac25-20fd-4e3d-b0d3-0d82c22aa04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from \"sentence\" column and store the result in \"filtered_sentence\" column\n",
    "remover = StopWordsRemover(inputCol=\"sentence\", outputCol=\"filtered_sentence\")\n",
    "textData = remover.transform(textData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc9820-cff0-4bf6-8090-ec3f483a3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataframe\n",
    "textData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ae324-bfc7-42b0-b9e8-acff3ecbe7e8",
   "metadata": {},
   "source": [
    "## Task 5 - StringIndexer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa58657-0e14-464a-93b7-14e7f995ff44",
   "metadata": {},
   "source": [
    "StringIndexer converts a column of strings into a column of integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b00245-6c58-4d84-b762-f89756a25144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import StringIndexer\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f27900-6ba8-4fc9-99eb-7129bce64399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with sample text and display it\n",
    "colors = spark.createDataFrame(\n",
    "    [(0, \"red\"), (1, \"red\"), (2, \"blue\"), (3, \"yellow\" ), (4, \"yellow\"), (5, \"yellow\")],\n",
    "    [\"id\", \"color\"])\n",
    "\n",
    "colors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db6b268-9b8b-4967-80f0-e8dd9d0a673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the strings in the column \"color\" and store their indexes in the column \"colorIndex\"\n",
    "indexer = StringIndexer(inputCol=\"color\", outputCol=\"colorIndex\")\n",
    "indexed = indexer.fit(colors).transform(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201f68e-d91c-4cbf-86d7-76a58e3693b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataframe\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4240d6-a15a-47fc-a873-a70c1967e403",
   "metadata": {},
   "source": [
    "## Task 6 - StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5562703d-40e8-4bcf-ae46-06310f5f27fd",
   "metadata": {},
   "source": [
    "StandardScaler transforms the data so that it has a mean of 0 and a standard deviation of 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90d48d-1ac0-406c-abc2-c5004eeeb79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import StandardScaler\n",
    "from pyspark.ml.feature import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98292b32-81e8-4b53-b994-c5c6d69db79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataframe and display it\n",
    "from pyspark.ml.linalg import Vectors\n",
    "data = [(1, Vectors.dense([70, 170, 17])),\n",
    "        (2, Vectors.dense([80, 165, 25])),\n",
    "        (3, Vectors.dense([65, 150, 135]))]\n",
    "df = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f49c4-244e-4f12-a0ac-df1a27e1104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the StandardScaler transformer\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c7ea6-7bc4-4863-94f8-5305e9a67ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the transformer to the dataset\n",
    "scalerModel = scaler.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3b12f-2543-4738-a49e-8e9229c079e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaledData = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e5b76-2731-427c-a870-a4243b823783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the scaled data\n",
    "scaledData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bbcd4-d0a2-4904-bc63-7f6a4c65f95e",
   "metadata": {},
   "source": [
    "Stop Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb10ede-ab3e-4d35-85b9-8c7ae09cb294",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa1902-20c5-449f-83df-0e1109a1e6ea",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaf1527-e4b5-40e0-90b9-78b15f666258",
   "metadata": {},
   "source": [
    "<!--\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2023-05-14|0.1|Ramesh Sannareddy|Initial Version Created|\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "prev_pub_hash": "4c21ca6799b5df9de8931185ed7970dda50592fa98e40cb08896d9bc6a728d0a"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
